{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train_data.csv')\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_encoding = {'CAG': 0, 'NAG': 1, 'OAG': 2}\n",
    "train_df['label'] = train_df['class_label'].map(class_encoding)\n",
    "test_df['label'] = test_df['class_label'].map(class_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[[\"comment\", \"label\"]]\n",
    "test_df = test_df[[\"comment\", \"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['comment', 'label'],\n",
       "        num_rows: 75\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['comment', 'label'],\n",
       "        num_rows: 90\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = 'ai4bharat/indic-bert'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt, keep_accents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch['comment'], padding='max_length', truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 75/75 [00:00<00:00, 3655.70 examples/s]\n",
      "Map: 100%|██████████| 90/90 [00:00<00:00, 6116.23 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_encoded = dataset.map(tokenize, batched=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from transformers import AutoModel\n",
    "model_ckpt = 'ai4bharat/indic-bert'\n",
    "device = torch.device('mps' if torch.cuda.is_available() else 'cpu')\n",
    "model = AutoModel.from_pretrained(model_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlbertConfig {\n",
      "  \"_name_or_path\": \"ai4bharat/indic-bert\",\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 200000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 75/75 [00:20<00:00,  3.67 examples/s]\n",
      "Map: 100%|██████████| 90/90 [00:23<00:00,  3.84 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def extract_hidden_states(batch):\n",
    "    inputs = {k:v.to(device) for k,v in batch.items()\n",
    "              if k in tokenizer.model_input_names}\n",
    "    with torch.no_grad():\n",
    "        last_hidden_state = model(**inputs).last_hidden_state\n",
    "    return{'hidden_state':last_hidden_state[:,0].cpu().numpy()}\n",
    "    \n",
    "dataset_encoded.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "dataset_hidden_states = dataset_encoded.map(extract_hidden_states, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from transformers import AutoModel\n",
    "device = torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu')\n",
    "# device = torch.device('cpu')\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = AutoModel.from_pretrained(model_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['comment', 'label', 'input_ids', 'token_type_ids', 'attention_mask', 'hidden_state'],\n",
       "        num_rows: 75\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['comment', 'label', 'input_ids', 'token_type_ids', 'attention_mask', 'hidden_state'],\n",
       "        num_rows: 90\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((75, 768), (90, 768))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Where hidden states will be the inputs and labels are the targets\n",
    "X_train = np.array(dataset_hidden_states['train']['hidden_state'])\n",
    "X_valid = np.array(dataset_hidden_states['test']['hidden_state'])\n",
    "y_train = np.array(dataset_hidden_states['train']['label'])\n",
    "y_valid = np.array(dataset_hidden_states['test']['label'])\n",
    "X_train.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_clf = LogisticRegression(max_iter=3000)\n",
    "lr_clf.fit(X_train,y_train)\n",
    "round(lr_clf.score(X_valid,y_valid),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM accuracy score: 0.389\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_clf = SVC()\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "svm_score = svm_clf.score(X_valid, y_valid)\n",
    "print(f\"SVM accuracy score: {round(svm_score, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest accuracy score: 0.478\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = RandomForestClassifier()\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "rf_score = rf_clf.score(X_valid, y_valid)\n",
    "print(f\"Random Forest accuracy score: {round(rf_score, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AlbertForSequenceClassification\n",
    "num_labels = 3\n",
    "model = (AlbertForSequenceClassification.from_pretrained(model_ckpt, num_labels=num_labels).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "def perf_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy': acc, 'f1 score': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "batch_size = 4 \n",
    "logging_steps = len(dataset_encoded['train']) // batch_size\n",
    "model_name = 'Custom Text Classifier'\n",
    "training_args = TrainingArguments(output_dir=model_name,\n",
    "                                  num_train_epochs=10, \n",
    "                                  learning_rate=5e-5,\n",
    "                                  per_device_train_batch_size=batch_size,\n",
    "                                  per_device_eval_batch_size=batch_size,\n",
    "                                  weight_decay=0.01,\n",
    "                                  evaluation_strategy='epoch',\n",
    "                                  disable_tqdm=False,\n",
    "                                  logging_steps=logging_steps,\n",
    "                                  log_level='error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                  args=training_args,\n",
    "                  compute_metrics=perf_metrics,\n",
    "                  train_dataset=dataset_encoded['train'],\n",
    "                  eval_dataset=dataset_encoded['test'],\n",
    "                  tokenizer=tokenizer,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 18/190 [00:22<03:21,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1015, 'grad_norm': 0.6538131237030029, 'learning_rate': 4.5263157894736846e-05, 'epoch': 0.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 19/190 [00:23<03:17,  1.15s/it]\n",
      " 10%|█         | 19/190 [00:33<03:17,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0977762937545776, 'eval_accuracy': 0.37777777777777777, 'eval_f1 score': 0.24951267056530213, 'eval_runtime': 9.4132, 'eval_samples_per_second': 9.561, 'eval_steps_per_second': 2.443, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 36/190 [00:53<03:04,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0963, 'grad_norm': 0.683247983455658, 'learning_rate': 4.0526315789473684e-05, 'epoch': 1.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 20%|██        | 38/190 [01:05<02:50,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.09470796585083, 'eval_accuracy': 0.35555555555555557, 'eval_f1 score': 0.2799452009978326, 'eval_runtime': 9.6232, 'eval_samples_per_second': 9.352, 'eval_steps_per_second': 2.39, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 54/190 [01:25<02:53,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0831, 'grad_norm': 3.0179920196533203, 'learning_rate': 3.578947368421053e-05, 'epoch': 2.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 30%|███       | 57/190 [01:38<02:37,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0703274011611938, 'eval_accuracy': 0.37777777777777777, 'eval_f1 score': 0.2697248381787684, 'eval_runtime': 9.7578, 'eval_samples_per_second': 9.223, 'eval_steps_per_second': 2.357, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 72/190 [01:57<02:29,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9609, 'grad_norm': 2.96803879737854, 'learning_rate': 3.105263157894737e-05, 'epoch': 3.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 40%|████      | 76/190 [02:12<02:13,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0513197183609009, 'eval_accuracy': 0.4666666666666667, 'eval_f1 score': 0.41517315187527953, 'eval_runtime': 10.2654, 'eval_samples_per_second': 8.767, 'eval_steps_per_second': 2.241, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 90/190 [02:33<02:36,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9128, 'grad_norm': 5.225546836853027, 'learning_rate': 2.6315789473684212e-05, 'epoch': 4.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 50%|█████     | 95/190 [02:57<03:35,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9911127686500549, 'eval_accuracy': 0.5111111111111111, 'eval_f1 score': 0.46614318053197507, 'eval_runtime': 14.1106, 'eval_samples_per_second': 6.378, 'eval_steps_per_second': 1.63, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 108/190 [03:22<02:42,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8147, 'grad_norm': 5.446675777435303, 'learning_rate': 2.1578947368421053e-05, 'epoch': 5.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 60%|██████    | 114/190 [03:49<02:25,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1856497526168823, 'eval_accuracy': 0.5111111111111111, 'eval_f1 score': 0.4958559188669348, 'eval_runtime': 14.6417, 'eval_samples_per_second': 6.147, 'eval_steps_per_second': 1.571, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▋   | 126/190 [04:13<02:10,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7838, 'grad_norm': 13.084046363830566, 'learning_rate': 1.6842105263157896e-05, 'epoch': 6.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 70%|███████   | 133/190 [04:42<01:55,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1436775922775269, 'eval_accuracy': 0.5666666666666667, 'eval_f1 score': 0.5748077215961012, 'eval_runtime': 14.2867, 'eval_samples_per_second': 6.3, 'eval_steps_per_second': 1.61, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 144/190 [05:05<01:37,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7678, 'grad_norm': 2.1491448879241943, 'learning_rate': 1.2105263157894737e-05, 'epoch': 7.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 80%|████████  | 152/190 [05:36<01:09,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2131448984146118, 'eval_accuracy': 0.5111111111111111, 'eval_f1 score': 0.5112956260497243, 'eval_runtime': 15.3702, 'eval_samples_per_second': 5.856, 'eval_steps_per_second': 1.496, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 162/190 [05:56<01:00,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5842, 'grad_norm': 3.872788667678833, 'learning_rate': 7.3684210526315784e-06, 'epoch': 8.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 90%|█████████ | 171/190 [06:28<00:36,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1523464918136597, 'eval_accuracy': 0.5111111111111111, 'eval_f1 score': 0.5112956260497243, 'eval_runtime': 15.1302, 'eval_samples_per_second': 5.948, 'eval_steps_per_second': 1.52, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 180/190 [06:47<00:23,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5264, 'grad_norm': 46.87462615966797, 'learning_rate': 2.631578947368421e-06, 'epoch': 9.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|██████████| 190/190 [07:24<00:00,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1298226118087769, 'eval_accuracy': 0.5333333333333333, 'eval_f1 score': 0.5293763799787896, 'eval_runtime': 15.9157, 'eval_samples_per_second': 5.655, 'eval_steps_per_second': 1.445, 'epoch': 10.0}\n",
      "{'train_runtime': 444.0019, 'train_samples_per_second': 1.689, 'train_steps_per_second': 0.428, 'train_loss': 0.8464475179973402, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=190, training_loss=0.8464475179973402, metrics={'train_runtime': 444.0019, 'train_samples_per_second': 1.689, 'train_steps_per_second': 0.428, 'total_flos': 17925348096000.0, 'train_loss': 0.8464475179973402, 'epoch': 10.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:15<00:00,  1.50it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 1.1298226118087769,\n",
       " 'test_accuracy': 0.5333333333333333,\n",
       " 'test_f1 score': 0.5293763799787896,\n",
       " 'test_runtime': 16.0848,\n",
       " 'test_samples_per_second': 5.595,\n",
       " 'test_steps_per_second': 1.43}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_output = trainer.predict(dataset_encoded['test'])\n",
    "preds_output.metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
